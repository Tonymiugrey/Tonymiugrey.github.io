<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Python与数据分析应用-Intro</title>
      <link href="/2022/11/29/Python%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%BA%94%E7%94%A8-Intro/"/>
      <url>/2022/11/29/Python%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%BA%94%E7%94%A8-Intro/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p>我经常跟别的专业的同学说，没事儿可以自己学学Py，以后打工的话帮助很大。这也是我暑假打工三个月之后最大的感受。</p><p>对于初学者来说，Py最方便的地方就是有很多第三方包。你可以理解为，别人把那些很复杂的功能写好后，打包成几句代码；我们不用去考虑背后复杂的逻辑、速度、资源占用等问题，直接拿来用就行。这个系列里，我会重点介绍pandas表格处理包和matplotlib画图包，也会提到其他一些诸如scikit_learn机器学习包等一些常用的包，并且讲讲快速上手这些包的方法——查文档。这里引用一句Howard的话：</p><img src="/2022/11/29/Python%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%BA%94%E7%94%A8-Intro/image-20221129012135276.png" class="" title="基本上是这样hhh"><p>关于用py和这些包到底能干点啥……先拿pandas举个简单的例子，我想把下面这个表格里所有包含空值的行全部删掉：</p><img src="/2022/11/29/Python%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%BA%94%E7%94%A8-Intro/image-20221012192926927.png" class="" title="红色框框标出来的就是缺失数值的单元格，我们要批量删掉他们"><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 第一步：导入表处理工具pandas. </span></span><br><span class="line"><span class="comment">#为了简化我们的代码，我们给他取个别名叫pd</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二步：用pandas打开一个Excel表格</span></span><br><span class="line">data = pd.read_excel(<span class="string">'房子.xlsx'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三步：删去所有包含空值的行</span></span><br><span class="line">data.dropna(axis=<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># dropna(Drop N/A) 就是用来删除空格的function, na就是空值的意思</span></span><br><span class="line"><span class="comment"># 括号里的是具体的一些参数，axis控制删除行/列，inplace控制是否替换原表格</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第四步：看看结果</span></span><br><span class="line">data</span><br></pre></td></tr></tbody></table></figure><p>运行之后得到结果：</p><img src="/2022/11/29/Python%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%BA%94%E7%94%A8-Intro/image-20221012193227179.png" class="" title="输出结果"><p>搞定！</p><p>可能这个例子所带来的视觉冲击感不太够，但如果你面对的是一个上百上千行的excel表格，有很多很多空值，你就知道为啥这东西好用了~ 因为你要做的依然是敲上面这几行代码！</p><p>而且你可能会注意到，上面这个表里有些数值错的离谱。你同样可以轻松地让Python在一堆数据中快速找到有问题的值进行修改，也可以将它所在的行/列直接删去，或者取相邻单元格的数值进行填充等等。</p><p>所以这玩意的难度下限其实很低，会一点英文，明白自己要干啥，文档/谷歌/百度在手，基本就没问题了；上限的话就看你打算怎么用它了，因为你可以结合不同的包，甚至自己造轮子，根据需求写出想要的功能（比如看看上面这个表中房价和户型、面积、楼层等变量之间的关系，根据这个关系得到一个预测房价的模型……），所以自由度非常高，当然难度一般也和任务的复杂度成正比。</p><p>看了这些，应该能大概感受到为啥这玩意儿能减轻工作量了吧！</p><p>那么接下来，就从准备工作开始做起吧！</p><h1 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h1><p>“环境”，你可以理解为电脑里专属Python的一个个小空间。一般来说，我们需要为每一个项目单独开一个环境，来保证之后安装的包们不会互相冲突……用人话说就是用房间把合不来的人隔开，防止他们打起来。</p><p>这里介绍一个叫做Anaconda的东西。你可以把它理解为一个精装修过的Python，自带了很多常用的包。他自带的默认环境叫base，里面装好了很多常用的包。当然，你也可以用它给你的项目开个专属的环境。</p><p>具体可以看看<a href="https://www.bilibili.com/video/BV1jK4y1k7wY">这个视频</a>的介绍。（偷个懒）</p><p>不过，我自己的习惯是平时随便写点东西，而且确定我不会装新的包的话，就不开新环境，直接在base环境里面写……虽然不是什么好习惯但是省事）</p><p>怎么安装Anaconda呢？直接去<a href="https://www.anaconda.com/">官网</a>下了装就好啦！墙内用户可以去<a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/?C=M&amp;O=A">清华的镜像站</a>找最新的那个文件下载就行，速度会快一些。</p><p>安装过程……再偷个懒：</p><p><a href="https://blog.csdn.net/shayuxing/article/details/122674859">Windows看这里</a></p><p><a href="https://blog.csdn.net/skeey111/article/details/124500951">Mac看这里</a></p><p><a href="https://zhuanlan.zhihu.com/p/84544848">Linux看这里</a>（你都用linux了……应该……也许……也不需要看这个了吧？ヽ(°▽、°)ﾉ）</p><hr><p>装好之后就可以愉快的和Python玩耍咯！</p><p>打开电脑里的Anaconda Prompt（Windows）或者“终端”（Mac/Linux），输入jupyter notebook然后回车，你就能进入Jupyter Notebook写点东西玩玩啦</p><p>如果你想进入base环境，可以在窗口里输入：</p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">conda activate base</span><br></pre></td></tr></tbody></table></figure><p>可以在<a href="%E6%88%BF%E5%AD%90-0.zip">这里</a>下载上面的那个例子。包含以下文件：</p><img src="/2022/11/29/Python%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%BA%94%E7%94%A8-Intro/image-20221129023450430.png" class="" title="文件"><p>ipynb文件就是Jupyter Notebook的文件。但是不能直接双击点开它，要先进到Jupyter Notebook才能打开它。Notebook默认进入的是User目录（Windows里面是C:/Users/你的用户名，Mac是个人文件夹，Linux…不懂user在哪儿还用啥linux =D），但是如果你想让它进入你指定的目录的话，可以按照以下操作进入目录后，再输入jupyter notebook启动它：</p><p>Mac/Linux：在终端里输入cd 空格 然后将文件夹拖进去（或者直接输入路径），回车</p><img src="/2022/11/29/Python%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%BA%94%E7%94%A8-Intro/image-20221129025134228.png" class="" title="出现你要的文件夹名称就成功啦（我的终端是美化过的，所以和原来的长得不一样）"><p>Windows：在Anaconda Prompt中输入文件所在盘符，比如F盘就输入：</p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">F:</span><br></pre></td></tr></tbody></table></figure><p>然后按照Mac那样来就行，cd 空格 想去的地方</p><img src="/2022/11/29/Python%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%BA%94%E7%94%A8-Intro/image-20221129025819738.png" class="" title="就这样"><h1 id="包安装"><a href="#包安装" class="headerlink" title="包安装"></a>包安装</h1><p>之后如果想安装其他包的话，直接在Prompt/终端里输入:</p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pip install xxx</span><br></pre></td></tr></tbody></table></figure><p>比如我想安装一个叫做“pygtrans”的包：</p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pip install pygtrans</span><br></pre></td></tr></tbody></table></figure><img src="/2022/11/29/Python%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%BA%94%E7%94%A8-Intro/image-20221129020526101.png" class="" title="安装成功"><p>如果看到这句话就说明安装成功啦！</p><p>这个包能够调用谷歌翻译进行批量翻译，超好用的。感兴趣可以看看它的<a href="https://pygtrans.readthedocs.io/zh_CN/latest/">文档</a>来了解它的用法。</p><p>上面提到的pip就是用来安装Python包的，但是由于服务器在海外，如果墙内用户觉得速度太慢的话可以换成<a href="https://mirrors.tuna.tsinghua.edu.cn/help/pypi/">清华的源</a>，按照里面的“设为默认“来操作就行啦。</p><h1 id="还要知道点啥……"><a href="#还要知道点啥……" class="headerlink" title="还要知道点啥……"></a>还要知道点啥……</h1><p>Python基础！我的建议是在开始学习怎么用包之前，先将基础打好，了解Python的语法、简单计算、数据的储存方式(list &amp; dict)、判断(if)、循环(for &amp; while)和递归（出新手村啦！）等。<em>（BA的小朋友们，没错，就是Howard的那门课的内容）</em></p><p>我将在另一个系列“数据结构与算法 with Python”里慢慢讲一些相关的内容如果你们等不及的话，可以去B站搜搜Python入门、基础之类的看看，或者去<a href="https://www.runoob.com/python3/python3-tutorial.html">菜鸟教程</a>看看，都挺不错的。</p><h1 id="接下来讲点啥……"><a href="#接下来讲点啥……" class="headerlink" title="接下来讲点啥……"></a>接下来讲点啥……</h1><p>之后每发一篇完会回来打个超链接的。也会随时补充忘记写的东西</p><ol><li>Pandas处理表格 - 定位、排序、去空值、统计</li><li>Pandas处理表格 - 分割字符串、替换、画图 (with matplotlib) and more</li><li>Matplotlib和Seaborn画图 - 散点图、柱状图、饼图、热力图 and more</li><li>表格类总结和拓展</li><li>NTLK与自然语言处理（NLP）</li><li>SciPy算概率 - 二项分布、Poisson分布和正态分布</li><li>线性回归：用多个包实现</li></ol>]]></content>
      
      
      <categories>
          
          <category> Python与应用 </category>
          
          <category> 数据分析应用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 数据分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SwiftUI自学 - 从一个菜谱APP开始</title>
      <link href="/2022/09/28/SwiftUI%E8%87%AA%E5%AD%A6+-+%E4%BB%8E%E4%B8%80%E4%B8%AA%E8%8F%9C%E8%B0%B1APP%E5%BC%80%E5%A7%8B/"/>
      <url>/2022/09/28/SwiftUI%E8%87%AA%E5%AD%A6+-+%E4%BB%8E%E4%B8%80%E4%B8%AA%E8%8F%9C%E8%B0%B1APP%E5%BC%80%E5%A7%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p>还是决定在大学的最后一年做点不一样的东西出来（至少和商学院里那群自以为是的“商业精英”相比…算是挺不一样的）</p><p>其实高考完买了一本讲Kotlin的安卓开发书，结果落灰到了现在笑死。后来觉得iOS的API毕竟都是为统一的硬件平台打造的，安卓那边好像还要考虑那些乱七八糟的硬件（尤其是相机），不管以后要做相机app还是AR相关的app，iOS都比安卓看起来好做一些……于是还是决定先从SwiftUI学起。</p><h1 id="什么是SwiftUI"><a href="#什么是SwiftUI" class="headerlink" title="什么是SwiftUI"></a>什么是SwiftUI</h1><p>苹果在2019年发布了SwiftUI，这是一套基于自家Swift语言打造的<strong>声明式UI框架</strong>，能够使用一套代码，打通iOS到macOS的所有苹果平台。你问我什么是“声明式”？其实我也不太会正经地解释他是啥……让我熟悉的Python（它应该算命令式编程语言吧）和SwiftUI一起煎个蛋吧：</p><p>Python:</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">鸡蛋 = 拿鸡蛋(冰箱=厨房里的冰箱, 哪个柜子=冷藏室, 第几层=第一层)</span><br><span class="line">荷包蛋 = 煎鸡蛋(鸡蛋, 火力=<span class="number">70</span>%, 戳破蛋黄=<span class="literal">True</span>, 调料=酱油)</span><br></pre></td></tr></tbody></table></figure><p>SwiftUI:</p><figure class="highlight swift"><table><tbody><tr><td class="code"><pre><span class="line">荷包蛋()</span><br><span class="line">.鸡蛋位置(冰箱<span class="operator">=</span>厨房里的冰箱, 哪个柜子<span class="operator">=</span>冷藏室, 第几层<span class="operator">=</span>第一层)</span><br><span class="line">.烹饪方法(火力<span class="operator">=</span><span class="number">70</span><span class="operator">%</span>, 戳破蛋黄<span class="operator">=</span><span class="literal">true</span>, 调料<span class="operator">=</span>酱油)</span><br></pre></td></tr></tbody></table></figure><p>不难发现，Python描述的是煎蛋的过程，通过过程得到我们想要的东西；而在SwiftUI这儿，我们需要做的是告诉它我们要什么，然后去定义这个东西的属性。个人感觉这种方式会更适合小白，或者没接触过图形界面开发的同学（比如我）</p><p>如果你只有iPad没有Mac，但是想体验一下用SwiftUI写app的话，可以去Store里下一个”Swift Playground”，里面有很多面向初学者的教程，也允许你用SwiftUI写一个简单的App；如果你注册了苹果的开发者计划（就是年费699的那个），是可以直接把你在iPad上写好的app上架到App Store的（交了这笔钱的有谁会用iPad写App啊）</p><p>如果你用SwiftUI想写一个功能更加复杂高级的app，那还是需要一部Mac的。这里给没有Mac的同学指几条路哈：</p><ol><li>如果你有很多钱：直接去苹果店买部Mac，建议一步到位上32G内存；</li><li>如果你有一些钱：可以去闲鱼看看所谓的”无头MacBook Pro“，一般都是没了屏幕的16或17款15寸MacBook Pro，价格在2000~3000之间，接个屏幕就能用，是你能够花最少钱就能体验到较为完整Mac体验的方式，但是他们的散热和CPU的性能确实有一些些的不太够用；</li><li>如果你和我一样没有钱，而且手上刚好有台装了Intel 7代~10代CPU的电脑，可以花点时间折腾一下黑苹果，如果折腾得好的话其实也还算挺稳定的。</li></ol><p>咳咳扯远了~</p><p>总之，这玩意给我的感觉还是挺不错的~个人感觉挺适合那种想学一点点代码的UI设计同学去玩玩。</p><h1 id="菜谱APP？"><a href="#菜谱APP？" class="headerlink" title="菜谱APP？"></a>菜谱APP？</h1><p>大概就是有次几个同学一起煮饭，但是不懂要煮啥，然后就有了做一个随机菜谱的想法，random到啥煮啥；然后就开始规划功能了：完全随机地出道菜、根据食材随机出菜、食材代购清单（最好能和外卖平台联动）、食材/菜品黑名单（忌口/不喜欢吃的菜）、菜品评分、自定义菜谱……</p><p>目前大概想到的就这些吧，之后再做着看看。</p><p>置于数据的来源……同学前两天发现了这个：<a href="https://cook.yunyoujun.cn/">https://cook.yunyoujun.cn</a> 看上去挺厉害的，之后准备联系作者问问它是怎么获得、处理这些数据的，如果能直接拿来用就更好hhh（不过个人的想法还是要在数据这块做点啥，毕竟老本行）</p><p>置于名字……那天吃饭忘了说到啥，开玩笑说把同学的胃存到云端变成云胃，然后突然就说，要不把云胃当做APP名？</p><p>那英文名就叫StomaCloud叭，中文名暂且叫云胃（有点蠢）</p><h1 id="老规矩，这篇不讲具体内容"><a href="#老规矩，这篇不讲具体内容" class="headerlink" title="老规矩，这篇不讲具体内容"></a>老规矩，这篇不讲具体内容</h1><p>下一篇开始讲讲我是怎么去设计软件初次启动的欢迎页的</p><img src="/2022/09/28/SwiftUI%E8%87%AA%E5%AD%A6+-+%E4%BB%8E%E4%B8%80%E4%B8%AA%E8%8F%9C%E8%B0%B1APP%E5%BC%80%E5%A7%8B/title.png" class="" title="">]]></content>
      
      
      <categories>
          
          <category> 移动应用开发 </category>
          
          <category> StomaCloud开发日记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SwiftUI </tag>
            
            <tag> 移动应用开发 </tag>
            
            <tag> StomaCloud开发日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构与算法 with Python - 入门和复健</title>
      <link href="/2022/09/28/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95+with+Python+-+%E5%85%A5%E9%97%A8%E5%92%8C%E5%A4%8D%E5%81%A5/"/>
      <url>/2022/09/28/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95+with+Python+-+%E5%85%A5%E9%97%A8%E5%92%8C%E5%A4%8D%E5%81%A5/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p>进入大学之后上得最开心的一门课 =w=</p><p>感觉还是要时不时把那些东西拿出来摸一摸的…那就做个备忘吧，这样以后随时都可以快速回顾一下。</p><p>这系列内容的前半部分将主要介绍几个常用数据类型和基础的算法，解释其原理并进行简单的应用，后半部分将主要复习一下数据结构与具体的应用。由于这个系列的目的是学会自己造轮子，涉及到很多原理上的东西，所以将<strong>完全不调用第三方库</strong>，仅仅使用用Python自带的一些库（其实也几乎用不上）</p><p><strong>–挖坑–</strong> 更多关于Python环境搭建、第三方包的安装与数据分析相关的内容会另开一个系列，毕竟也是很实用的一项技能，再加上平时应用的场景不多，自己其实也算不上特别熟练（在实习的时候感觉尤为明显）</p><h1 id="题外话"><a href="#题外话" class="headerlink" title="题外话"></a>题外话</h1><p>进入BA之后，感觉自己复活了 -w-</p><p>能够再次学上自己喜欢的东西，从传统商科的苦海中逃离，清楚自己在学什么、做什么，和一群踏实靠谱的同学和老师相处……在学校里已经很久没有这种舒畅、充实、踏实的感觉了……</p><p>所以这一年，是我大学三年来最棒的一年。</p><p>在这里感谢一下教我这门课的恩师Howard！Hoho是我在大学3年里遇见过最有意思的老师。很难想像有机会在死气沉沉的商学院里能遇见一位会和我们一起玩、一起折腾，而且兴趣相仿的老师，也让我对未来的想法更加清晰了一些。虽然最后考得不太好……但是真的从Ho的课里收获了很多。作为一个非CS专业，一直靠自学的小白，Ho的课让我更加了解如何去让代码变得高效、规范、易读，让我有机会了解到项目开发的流程与所需的能力，也让我更加清楚如何更高效地去提升自己这方面的能力；更重要的是，不论是风格逆天却又不难的卷子，还是平时的相处，在Ho的影响下，我渐渐地能够更好地去调整自己的心态，更加大胆地去放弃与止损。比起一些专业技能与能力，这样的收获是更加珍贵难得的。</p><p>有机会的话，毕业前一起带相机出来拍拍照哈！</p><p>总而言之，Ho真的是一位很不一样的Professor！很感谢这一年来的帮助与指点！</p><h1 id="系列内容"><a href="#系列内容" class="headerlink" title="系列内容"></a>系列内容</h1><p>这篇就先不写具体内容了吧，就当开个头，规划一下要写点啥。之后每发一篇完会回来打个超链接的。</p><ol><li>Python中常用的语法</li><li>Python中常用的数据类型与特点</li><li>一些Python自带小功能</li><li>字符串的处理</li><li>自定义函数</li><li>啃下递归</li><li>算法1 - 搜索算法</li><li>算法2 - 排序算法</li><li>算法？ - 待补充</li><li>数据结构1 - Queue &amp; Stack</li><li>数据结构2 - 单链表</li><li>数据结构3 - 图</li><li>数据结构4 - 树</li></ol><p>（刚刚在动车上一边翻当时上课用的notebooks，一边不自觉嘴角上扬，像是有什么大病）</p>]]></content>
      
      
      <categories>
          
          <category> Python与应用 </category>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DL和PyTorch入门 - BP神经网络（一）</title>
      <link href="/2022/07/16/DL%E5%92%8CPyTorch%E5%85%A5%E9%97%A8+-+%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2022/07/16/DL%E5%92%8CPyTorch%E5%85%A5%E9%97%A8+-+%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p>大三上的后半个学期接触到GAN之后，突然开始对深度学习开始感兴趣，自己也开始多多少少了解了点相关的知识，玩了点现成的代码，甚至想过接触一下CV和计算摄影的相关领域。后来发现自己的脑子似乎不太支持自己去深入研究这些东西，于是还是决定把它当做一个工具，自己蛮学一学。</p><p>大三下DM的大作业是：在ML和DL领域选择一种算法，实现具体应用，并进行优化。优化算法这种事情我们显然是搞不来，于是选择在研究方法上进行创新。（虽然最后创了个我到现在还觉得很离谱的方法出来）借此机会，总算是开始独立使用PyTorch搭建神经网络并进行调参。</p><p>所以最开始，还是先来回顾一下一个最基本的BP神经网络搭建的过程。本文侧重于训练的过程与PyTorch的使用上，而非BP神经网络的详细原理。之后有有机会复习ML算法的话再聊聊吧（怎么又给自己挖坑了）</p><p>在学校借了一本PyTorch的书，虽然有点老但是也还算可以凑活着看。在9月份还书之前慢慢啃一点吧。</p><h1 id="Content"><a href="#Content" class="headerlink" title="Content"></a>Content</h1><ol><li>构建一个神经网络</li><li>训练参数的准备</li><li>训练数据的准备</li><li>训练过程的实现</li><li>梯度下降的优化</li><li>最后的代码包装</li></ol><h2 id="构建一个神经网络"><a href="#构建一个神经网络" class="headerlink" title="构建一个神经网络"></a>构建一个神经网络</h2><p>在Torch中定义网络是一件很简单的事情，只需要用到torch.nn中的 Sequential() 便可以定义一个网络中每一层的具体类型与输入输出。这里以一个简单的三层神经网络为例，包含线性的输入与输出层，以及一个使用ReLU作为激活函数的隐层。Torch囊括了主流的激活函数，如Sigmoid, tanh和RuLU等。通常，为了更快的收敛，我们会在隐层使用RuLU函数。</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(input_data, hidden_layer), <span class="comment"># 输入层到隐层</span></span><br><span class="line">    torch.nn.ReLU(), <span class="comment"># 隐层</span></span><br><span class="line">    torch.nn.Linear(hidden_layer, output_data) <span class="comment"># 隐层到输出层</span></span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure><p>这里我们将输入节点个数（也就是特征个数）设为1个，隐层节点个数设为10个，最后输出的节点个数为1个：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">input_layer = <span class="number">1</span></span><br><span class="line">hidden_layer = <span class="number">10</span></span><br><span class="line">output_layer = <span class="number">1</span></span><br></pre></td></tr></tbody></table></figure><p>这样我们就得到了一个复杂度非常低的网络。低到离谱。</p><p>节点的个数是影响网络性能的参数之一。对于输入输出层，其节点个数往往由我们的数据决定；对于隐层，我们通常可以根据以下公式来确定其个数：<br>$$<br>(input + ouput)^2 + radn(0,10)<br>$$<br>接下来，我们需要定义这个网络的损失函数。torch.nn中支持的损失函数类型有MSELoss, L1Loss和CrossEntropyLoss等，各自适用于处理不同类型的问题。可以在这里找到更多详细的信息：<a href="https://www.w3cschool.cn/article/78828381.html">pytorch中常用的损失函数用法说明 | w3c笔记 (w3cschool.cn)</a>。当然去官方的Document找找看肯定是更完整准确的。</p><p>这里我们先以MSE作为我们的损失函数：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">loss_fn = torch.nn.MSELoss()</span><br></pre></td></tr></tbody></table></figure><p>之后将预测值与实际值传入即可计算损失：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">loss = loss_fn(y_pred, y)</span><br></pre></td></tr></tbody></table></figure><p>至此，一个神经网络的基本框架搭建完毕，下一步我们便可以开始为训练做准备。以下是第一部分的完整代码：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">input_data = <span class="number">1</span></span><br><span class="line">hidden_layer = <span class="number">10</span></span><br><span class="line">output_data = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(input_layer, hidden_layer), <span class="comment"># 输入层到隐层</span></span><br><span class="line">    torch.nn.ReLU(), <span class="comment"># 隐层</span></span><br><span class="line">    torch.nn.Linear(hidden_layer, output_layer) <span class="comment"># 隐层到输出层</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">loss_fn = torch.nn.MSELoss()</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h2 id="训练参数的准备"><a href="#训练参数的准备" class="headerlink" title="训练参数的准备"></a>训练参数的准备</h2><p>基于BPNN的原理，我们需要在训练时完成误差的计算，并根据我们定义的学习率，来控制每一次训练中梯度下降的步长。学习率的控制是一门很复杂的学问，不过一般来说，只要不出现步子迈大了收敛不了的情况就行，但是过小的学习率也会导致收敛过慢。</p><p>这里我们定义学习率为1e-4：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">lr = <span class="number">1e-4</span></span><br></pre></td></tr></tbody></table></figure><p>训练时，Batch（批）与Epoch（次）也是调起来很玄学的两个参数，影响着训练时间与训练结果的准确性。当整个训练集经过网络处理一次后，我们将这个过程成为一个epoch；由于一个epoch中处理样本可能过于庞大，我们需要将其分为多个小块，也就是batches. 每个batch中的内容从训练集中随机抽取，使用小部分样本对模型权重新进更新。训练一个batch的过程，就是我们常听的“一次Iteration（迭代）”。</p><p>那么Epoch和Batch是如何影响训练结果的呢？</p><p>Batch size主要决定了收敛速度与随机梯度的噪声，太大不好，太小也不好。太小会延缓训练时间，并且导致过拟合；太大会导致梯度下降得不够到位，误差过大，同时占用大量的内存。（曾经在尝试玩GAN的时候经常因为batch太大炸显存）所以需要经过多次尝试来确定batch size.</p><p>Epoch主要影响拟合的程度。使用多个Epoch的原因在于，仅有一次完整的训练往往会导致欠拟合，但过多的Epoch也会导致过拟合。</p><p>那么，究竟要怎么取到合适的Batch size和epoch呢？</p><p>其实没有正确的答案，因为对于不同的训练集，我们使用的参数往往是不同的。通常来说，batch size一般不会低于16，我们可以根据这个限制，与样本的大小，去确定batch size；对于epoch的数量，emmm…反正我是自己感受的hhhhh，用很大的值试一试，很小的的值也试一试，根据训练的情况去选择一个合适的值。</p><p>反正只要避免出问题就好了。真的很玄学。</p><p>代码实现上，epoch只要用for循环来控制就好了，batch则需要写一个专门用来分割的小东西，以将划分好的batches返回进训练过程中:</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">batch_loader</span>(<span class="params">x, y, batch_size=<span class="number">64</span></span>):</span><br><span class="line">    <span class="string">""" </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        x: 训练数据 Tensor or ndarray</span></span><br><span class="line"><span class="string">        y: 训练数据标签 Tensor or ndarray</span></span><br><span class="line"><span class="string">        batch_size: Mini-batch 大小</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        x_batch, y_batch: mini-batch 数据对</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data_size = x.shape[<span class="number">0</span>]</span><br><span class="line">    permutation = np.random.permutation(data_size)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, data_size, batch_size):</span><br><span class="line">        batch_permutation = permutation[i: i+batch_size]</span><br><span class="line">        <span class="keyword">yield</span> x[batch_permutation], y[batch_permutation]</span><br></pre></td></tr></tbody></table></figure><p>(CtrlC CtrlV来的)</p><p>Epoch我们暂定给1000个吧，batch size给个100看看.</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">num_epoch = <span class="number">1000</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br></pre></td></tr></tbody></table></figure><h2 id="训练数据的准备"><a href="#训练数据的准备" class="headerlink" title="训练数据的准备"></a>训练数据的准备</h2><p>正常来说，我们需要将数据导入，并进行训练集与测试集的分割。这里为了省点事情，我们就暂且不进行划分。我们使用以下公式，随机生成一组0~1之间的随机数作为Y，并引入一些噪声：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">F</span>(<span class="params">x</span>):</span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(x))</span><br><span class="line"></span><br><span class="line">X_BOUND = [-<span class="number">10</span>, <span class="number">10</span>]</span><br><span class="line">data_length = <span class="number">1000</span></span><br><span class="line">noise = <span class="number">0.1</span></span><br><span class="line">x = X_BOUND[<span class="number">0</span>] + (X_BOUND[<span class="number">1</span>]-X_BOUND[<span class="number">0</span>]) * np.random.rand(data_length)</span><br><span class="line">y = F(x) + noise * np.random.rand(data_length)</span><br></pre></td></tr></tbody></table></figure><img src="/2022/07/16/DL%E5%92%8CPyTorch%E5%85%A5%E9%97%A8+-+%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/data_graph.png" class="" title="我们的数据分布"><p>接着，我们需要将其包装为tensor（张量）：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x_tensor = torch.from_numpy(x).unsqueeze_(-<span class="number">1</span>).<span class="built_in">float</span>()</span><br><span class="line">y_tensor = torch.from_numpy(y).unsqueeze_(-<span class="number">1</span>).<span class="built_in">float</span>()</span><br></pre></td></tr></tbody></table></figure><p>正常来说，这一步需要做的事情很多：如果是现实中的数据，我们还需要进行归一化的处理，以消除各变量之间量级差距所带来的误差；如果是分类变量，我们需要将每一种分类独立出来，作为一个0/1的dummy变量，等等。之后我会在下一篇DL文章中，用我当时大作业的数据进行演示。</p><h2 id="训练过程的实现"><a href="#训练过程的实现" class="headerlink" title="训练过程的实现"></a>训练过程的实现</h2><p>刚刚提到，我们将会用for循环来实现对epoches的训练，因此我们需要将具体的训练过程装进这个for中。训练一个BP神经网络的流程大致包含了误差计算、反向传播与梯度计算，更新权重这几个步骤。这些步骤都可以很轻易的用PyTorch实现。</p><p>首先，我们需要建立一个用于记录误差的列表，以便我们在每次迭代后能够记录当前的损失，并在训练完一个epoch之后进行平均，供我们评估模型的性能。当然，在这之后，需要清空这个列表，开始新的一轮训练。</p><p>这是一个大框架：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">loss_list = []  <span class="comment"># 存放每次训练误差的list</span></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):  <span class="comment"># 控制epoch</span></span><br><span class="line">loss_list.clear()  <span class="comment"># 清空列表</span></span><br><span class="line"><span class="keyword">for</span> i, (x_batch, y_batch) <span class="keyword">in</span> <span class="built_in">enumerate</span>(batch_loader(x_tensor, y_tensor, batch_size)):  <span class="comment"># batch</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">pass</span>  <span class="comment"># 具体的训练步骤</span></span><br><span class="line"></span><br><span class="line">loss_list.append( <span class="comment"># loss ) # 记录该次误差</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(np.mean(loss_list)) <span class="comment"># 误差求平均</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>接下来，我们需要将具体的训练内容放进for中。</p><p>第一件事情，当然是将x输入网络中，进行预测。</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">y_pred = model(x_batch) <span class="comment"># 预测</span></span><br></pre></td></tr></tbody></table></figure><p>接着，我们用刚刚提到过的那句代码计算损失，并执行反向传播：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">loss = loss_fn(y_pred, y_batch) <span class="comment"># 计算损失</span></span><br><span class="line">model.zero_grad() <span class="comment"># 将模型中参数的梯度设为0</span></span><br><span class="line">loss.backward() <span class="comment"># 反向传播</span></span><br></pre></td></tr></tbody></table></figure><p>要想实现梯度下降并更新权重，我们需要从模型中提取参数，并执行更新的公式：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">param.data -= param.grad.data * lr</span><br></pre></td></tr></tbody></table></figure><p>紧接着记录一下本次迭代的误差：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">loss_list.append(loss.item())</span><br></pre></td></tr></tbody></table></figure><p>在训练完一个epoch之后，计算一下MSE：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">np.mean(loss_recorder)</span><br></pre></td></tr></tbody></table></figure><p>至此，我们已经初步搭建好训练的逻辑与步骤了。以下是完整代码：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># import torch</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batch_loader</span>(<span class="params">x, y, batch_size=<span class="number">64</span></span>):</span><br><span class="line">    <span class="string">""" </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        x: 训练数据 Tensor or ndarray</span></span><br><span class="line"><span class="string">        y: 训练数据标签 Tensor or ndarray</span></span><br><span class="line"><span class="string">        batch_size: Mini-batch 大小</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        x_batch, y_batch: mini-batch 数据对</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data_size = x.shape[<span class="number">0</span>]</span><br><span class="line">    permutation = np.random.permutation(data_size)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, data_size, batch_size):</span><br><span class="line">        batch_permutation = permutation[i: i+batch_size]</span><br><span class="line">        <span class="keyword">yield</span> x[batch_permutation], y[batch_permutation]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">F</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(x))</span><br><span class="line"></span><br><span class="line">X_BOUND = [-<span class="number">10</span>, <span class="number">10</span>]</span><br><span class="line">data_length = <span class="number">1000</span></span><br><span class="line">noise = <span class="number">0.1</span></span><br><span class="line">x = X_BOUND[<span class="number">0</span>] + (X_BOUND[<span class="number">1</span>]-X_BOUND[<span class="number">0</span>]) * np.random.rand(data_length)</span><br><span class="line">y = F(x) + noise * np.random.rand(data_length)</span><br><span class="line"></span><br><span class="line">x_tensor = torch.from_numpy(x).unsqueeze_(-<span class="number">1</span>).<span class="built_in">float</span>()</span><br><span class="line">y_tensor = torch.from_numpy(y).unsqueeze_(-<span class="number">1</span>).<span class="built_in">float</span>()</span><br><span class="line">        </span><br><span class="line">input_layer = <span class="number">1</span></span><br><span class="line">hidden_layer = <span class="number">10</span></span><br><span class="line">output_layer = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(input_layer, hidden_layer), <span class="comment"># 输入层到隐层</span></span><br><span class="line">    torch.nn.ReLU(), <span class="comment"># 隐层</span></span><br><span class="line">    torch.nn.Linear(hidden_layer, output_layer) <span class="comment"># 隐层到输出层</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">loss_fn = torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line">num_epoch = <span class="number">1000</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">lr = <span class="number">1e-3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line">loss_list = []  <span class="comment"># 存放每次训练误差的list</span></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):  <span class="comment"># 控制epoch</span></span><br><span class="line">    loss_list.clear()  <span class="comment"># 清空列表</span></span><br><span class="line">    <span class="keyword">for</span> i, (x_batch, y_batch) <span class="keyword">in</span> <span class="built_in">enumerate</span>(batch_loader(x_tensor, y_tensor, batch_size)):  <span class="comment"># batch</span></span><br><span class="line">        y_pred = model(x_batch) <span class="comment"># 预测</span></span><br><span class="line">        loss = loss_fn(y_pred, y_batch) <span class="comment"># 计算损失</span></span><br><span class="line">        model.zero_grad() <span class="comment"># 将模型中参数的梯度设为0</span></span><br><span class="line">        loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新权重</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">            param.data -= param.grad.data * lr</span><br><span class="line">            </span><br><span class="line">        loss_list.append(loss.item())</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'batch {} loss {}'</span>.<span class="built_in">format</span>(i+<span class="number">1</span>, loss.item()), end=<span class="string">'\r'</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'epoch {} loss {}'</span>.<span class="built_in">format</span>(e+<span class="number">1</span>, np.mean(loss_list)))</span><br></pre></td></tr></tbody></table></figure><p>训练完毕之后，我们可以通过输出的结果看到误差不断减少的过程。如果想直观地看到训练成果，我们可以将原数据集传入训练完毕的模型中进行预测，并将原数据的曲线和预测结果的曲线一起plot出来进行对比：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># Plot</span></span><br><span class="line">x_axis = np.linspace(*X_BOUND, <span class="number">200</span>)</span><br><span class="line">plt.plot(x_axis, F(x_axis))</span><br><span class="line">plt.scatter(x, np.squeeze(</span><br><span class="line">    model(x_tensor).detach().cpu().numpy(), -<span class="number">1</span>), color=<span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><img src="/2022/07/16/DL%E5%92%8CPyTorch%E5%85%A5%E9%97%A8+-+%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/compare_graph_sdg.png" class="" title="Epoch = 1000, Batch size = 100, Learning rate = 1e-3"><p>红色曲线为预测结果，蓝色曲线为原数据，可以看到还是很接近的。如果我们增加epoch、提高learning rate，或是增大batch size，训练误差也会随之减小。</p><img src="/2022/07/16/DL%E5%92%8CPyTorch%E5%85%A5%E9%97%A8+-+%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/compare_graph_sdg_more_epoch.png" class="" title="Epoch = 5000, Batch size = 100, Learning rate = 1e-3"><p>当然，这只是训练误差，并不代表模型真正的预测性能。想要正确评判模型的预测性能，还是需要引入测试集进行验证。之后再说啦~</p><h2 id="梯度下降的优化"><a href="#梯度下降的优化" class="headerlink" title="梯度下降的优化"></a>梯度下降的优化</h2><p>某种意义上来说，BP神经网络是一种局部搜索算法，这使他在训练过程中不可避免地会陷入局部最优，从而导致训练结果不佳。此外，收敛速度慢也是它的缺点之一。为了解决这些问题，我们可以从两方面入手：优化初始权重 ，与优化梯度下降的算法。前者通常与在GA、PSO等算法的辅助下进行，之后有机会也会展示一下具体的做法；后者则可以利用PyTorch自带的Optimizer (torch.optim) 进行优化。torch.optim中包含了多种优化算法，如Adam, AdamW 和 Adamax等，当然也包含了我们刚刚的随机梯度下降方法SGD。若要使用torch.optim, 我们需要确定一种优化算法，然后将模型的参数与学习率传进去：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=lr)</span><br></pre></td></tr></tbody></table></figure><p>同时，将刚刚训练过程中的部分代码进行替换： </p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># model.zero_grad()</span></span><br><span class="line">optimizer.zero_grad() <span class="comment"># 将模型中参数的梯度设为0</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">for param in model.parameters():</span></span><br><span class="line"><span class="string">param.data -= param.grad.data * lr</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">optimizer.step() <span class="comment"># 更新权重</span></span><br></pre></td></tr></tbody></table></figure><p>再次执行代码，我们可以得到一个和之前性能几乎相同的模型。</p><p>如果此时我们将SGD换为Adam，并依然使用Epoch = 1000, Batch_size = 100, Learning_rate = 1e-3这套参数：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=lr)</span><br></pre></td></tr></tbody></table></figure><img src="/2022/07/16/DL%E5%92%8CPyTorch%E5%85%A5%E9%97%A8+-+%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/compare_graph_adam.png" class="" title="Epoch = 1000, Batch size = 100, Learning rate = 1e-3, with Adam optimizer"><p>一步到位。</p><p>即使此时我们将Learning_rate减小到1e-4：</p><img src="/2022/07/16/DL%E5%92%8CPyTorch%E5%85%A5%E9%97%A8+-+%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/compare_graph_adam_less_lr.png" class="" title="Epoch = 1000, Batch size = 100, Learning rate = 1e-4, with Adam optimizer"><p>结果还是会比刚刚SGD好不少的。</p><h2 id="最后的代码包装"><a href="#最后的代码包装" class="headerlink" title="最后的代码包装"></a>最后的代码包装</h2><p>至此，我们已经得到了一个最简单的神经网络模型，并初步体验了调参的……呃用什么形容词词好呢……反正就是一件神奇又折磨人的事情……</p><p>接来下我们要做的，就是把我们的代码包装成一个类，方便我们之后的调用。直接上代码：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BPNN</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_layer, hidden_layer, output_layer, x, y, epoch, batch_size, lr, log</span>):</span><br><span class="line">        self.input_layer = input_layer  <span class="comment"># 输入层节点个数</span></span><br><span class="line">        self.hidden_layer = hidden_layer  <span class="comment"># 隐层节点个数</span></span><br><span class="line">        self.output_layer = output_layer  <span class="comment"># 输出层节点个数</span></span><br><span class="line">        self.x = x  <span class="comment"># 自变量</span></span><br><span class="line">        self.y = y  <span class="comment"># 因变量</span></span><br><span class="line">        self.epoch = epoch</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.lr = lr  <span class="comment"># Learning Rate</span></span><br><span class="line">        self.log = log  <span class="comment"># 是否输出记录，T/F</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">batch_loader</span>(<span class="params">self, x, y, batch_size=<span class="number">64</span></span>):</span><br><span class="line">        data_size = x.shape[<span class="number">0</span>]</span><br><span class="line">        permutation = np.random.permutation(data_size)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, data_size, batch_size):</span><br><span class="line">            batch_permutation = permutation[i: i + batch_size]</span><br><span class="line">            <span class="keyword">yield</span> x[batch_permutation], y[batch_permutation]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_model</span>(<span class="params">self</span>):</span><br><span class="line">        input_layer = self.input_layer</span><br><span class="line">        hidden_layer = self.hidden_layer</span><br><span class="line">        output_layer = self.output_layer</span><br><span class="line"></span><br><span class="line">        model = torch.nn.Sequential(</span><br><span class="line">            torch.nn.Linear(input_layer, hidden_layer),  <span class="comment"># 输入层到隐层</span></span><br><span class="line">            torch.nn.ReLU(),  <span class="comment"># 隐层</span></span><br><span class="line">            torch.nn.Linear(hidden_layer, output_layer)  <span class="comment"># 隐层到输出层</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        loss_fn = torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> model, loss_fn</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train_model</span>(<span class="params">self</span>):</span><br><span class="line">        num_epoch = self.epoch</span><br><span class="line">        batch_size = self.batch_size</span><br><span class="line">        lr = self.lr</span><br><span class="line">        x = self.x</span><br><span class="line">        y = self.y</span><br><span class="line"></span><br><span class="line">        model, loss_fn = self.build_model()</span><br><span class="line"></span><br><span class="line">        optimizer = torch.optim.Adam(model.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">        loss_list = []  <span class="comment"># 存放每次训练误差的list</span></span><br><span class="line">        <span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):  <span class="comment"># 控制epoch</span></span><br><span class="line">            loss_list.clear()  <span class="comment"># 清空列表</span></span><br><span class="line">            <span class="keyword">for</span> i, (x_batch, y_batch) <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.batch_loader(x, y, batch_size)):  <span class="comment"># batch</span></span><br><span class="line">                y_pred = model(x_batch)  <span class="comment"># 预测</span></span><br><span class="line">                loss = loss_fn(y_pred, y_batch)  <span class="comment"># 计算损失</span></span><br><span class="line">                optimizer.zero_grad()  <span class="comment"># 将优化器中参数的梯度设为0</span></span><br><span class="line">                loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">                optimizer.step()  <span class="comment"># 更新权重</span></span><br><span class="line">                loss_list.append(loss.item())</span><br><span class="line">                <span class="keyword">if</span> self.log:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">'batch {} loss {}'</span>.<span class="built_in">format</span>(i + <span class="number">1</span>, loss.item()), end=<span class="string">'\r'</span>)</span><br><span class="line">            <span class="keyword">if</span> self.log:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">'epoch {} loss {}'</span>.<span class="built_in">format</span>(e + <span class="number">1</span>, np.mean(loss_list)))</span><br><span class="line">        <span class="keyword">return</span> model, np.mean(loss_list)</span><br><span class="line">    </span><br></pre></td></tr></tbody></table></figure><p>将以上代码保存为BPNN.py, 然后在同一目录下新建一个.py文件，调用BPNN.py写好主程序:</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> BPNN <span class="keyword">import</span> BPNN</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">F</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    X_BOUND = [-<span class="number">10</span>, <span class="number">10</span>]</span><br><span class="line">    data_length = <span class="number">1000</span></span><br><span class="line">    noise = <span class="number">0.1</span></span><br><span class="line">    x = X_BOUND[<span class="number">0</span>] + (X_BOUND[<span class="number">1</span>] - X_BOUND[<span class="number">0</span>]) * np.random.rand(data_length)</span><br><span class="line">    y = F(x) + noise * np.random.rand(data_length)</span><br><span class="line"></span><br><span class="line">    x_tensor = torch.from_numpy(x).unsqueeze_(-<span class="number">1</span>).<span class="built_in">float</span>()</span><br><span class="line">    y_tensor = torch.from_numpy(y).unsqueeze_(-<span class="number">1</span>).<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define</span></span><br><span class="line">    MyBPNN = BPNN(input_layer=<span class="number">1</span>, hidden_layer=<span class="number">10</span>, output_layer=<span class="number">1</span>,</span><br><span class="line">                  x=x_tensor, y=y_tensor,</span><br><span class="line">                  epoch=<span class="number">1000</span>, batch_size=<span class="number">100</span>, lr=<span class="number">1e-4</span>,</span><br><span class="line">                  log=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Train</span></span><br><span class="line">    model, loss = MyBPNN.train_model()  <span class="comment"># 将</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'MSE: '</span>, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot</span></span><br><span class="line">    x_axis = np.linspace(*X_BOUND, <span class="number">200</span>)</span><br><span class="line">    plt.plot(x_axis, F(x_axis))</span><br><span class="line">    plt.scatter(x, np.squeeze(</span><br><span class="line">        model(x_tensor).detach().cpu().numpy(), -<span class="number">1</span>), color=<span class="string">'r'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h1 id="挖坑"><a href="#挖坑" class="headerlink" title="挖坑"></a>挖坑</h1><p>BP神经网路部分的下一篇文章会将以上神经网络应用在空气质量预测上（数据源：<a href="https://archive-beta.ics.uci.edu/ml/datasets/beijing+multi+site+air+quality+data">Beijing Multi-Site Air-Quality Data - UCI Machine Learning Repository</a>），加入数据的预处理、训练集测试集的划分，以及最后的验证上。</p><p>下下篇文章则会将网络与GA、PSO算法结合（使用 <a href="https://scikit-opt.github.io/scikit-opt/#/zh/README">scikit-opt</a>），实现对神经网络性能的进一步优化。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p><a href="https://www.pytorch123.com/SecondSection/neural_networks/">PyTorch 神经网络 - PyTorch官方教程中文版 (pytorch123.com)</a></p><p><a href="https://github.com/cattidea/bp-ga-pytorch">cattidea/bp-ga-pytorch: bp-ga algorithm implemented by pytorch (github.com)</a></p><p><a href="https://blog.csdn.net/weixin_38754799/article/details/109831970">(20条消息) 训练神经网络 | 三个基本概念：Epoch, Batch, Iteration_OnlyCoding…的博客-CSDN博客</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习与深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World!</title>
      <link href="/2022/06/25/hello-world/"/>
      <url>/2022/06/25/hello-world/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Hi! 我是miuGrey，这是我的第二个Hexo博客。至于第一个……源文件随着一次macOS的重装没了……TT（好在里面也没多少东西hh）</p><p>该博客的主题为<a href="https://butterfly.js.org/">Butterfly</a>. 鉴于大一的时候还会折腾，但现在一点也不记得了……之后我会把详细的配置过程记录一下当个备忘。如果不懒的话，之后可能会对这个主题进行一些改动。</p><p>打算用这个博客更新一些学习记录，也会随便写点评测之类的东西。</p><img src="/2022/06/25/hello-world/image-20220625145549064.png" class="" title="image-20220625145549064">]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
